# Spark Package Overview

This document describes everything inside the `spark` feature module in your backend project. It acts as a complete reference for Copilot or future contributors.

---

# ğŸŒŸ Spark Feature Overview

A **Spark** is a unit of thought/idea in Ignitr.  
A Spark can be:

- A **top-level spark** (root)
- A **child spark** (nested)
- A node in a recursive **tree of sparks**

Each spark contains:

- id  
- title  
- description  
- parentId (nullable)
- createdAt  
- updatedAt  

Mongo persists all sparks in a single collection.

---

# ğŸ“¦ Package Structure

```
spark/
â”œâ”€â”€ controller/
â”‚   â””â”€â”€ SparkController.java
â”œâ”€â”€ service/
â”‚   â”œâ”€â”€ SparkService.java
â”‚   â””â”€â”€ SparkServiceImpl.java
â”œâ”€â”€ dto/
â”‚   â”œâ”€â”€ CreateSparkRequestDTO.java
â”‚   â”œâ”€â”€ UpdateSparkRequestDTO.java
â”‚   â”œâ”€â”€ SparkDTO.java
â”‚   â””â”€â”€ SparkTreeDTO.java
â”œâ”€â”€ model/
â”‚   â””â”€â”€ Spark.java
â”‚   â””â”€â”€ SparkDeleteMode.java
â”œâ”€â”€ repository/
â”‚   â””â”€â”€ SparkRepository.java
â”œâ”€â”€ mapper/
â”‚   â””â”€â”€ SparkMapper.java
â””â”€â”€ exception/
    â”œâ”€â”€ SparkNotFoundException.java
    â””â”€â”€ SparkAlreadyExistsException.java
```

---

# ğŸ§© DTO Layer

### 1ï¸âƒ£ CreateSparkRequestDTO  
- Used for creating both top-level and child sparks.

### 2ï¸âƒ£ UpdateSparkRequestDTO  
- Used for updating spark title and description.

### 3ï¸âƒ£ SparkDTO  
- Returned by all non-tree endpoints.

### 4ï¸âƒ£ SparkTreeDTO  
- Returned only by `/api/sparks/{id}/tree`.
- Contains recursive children list.

---

# ğŸ§¬ Entity Layer

### Spark.java
Mongo document stored in the database.

Contains:
- id
- title
- description
- parentId
- createdAt
- updatedAt

No controller-specific info.

---

# ğŸ” Mapper Layer

### SparkMapper  
Contains all mapping logic:

- `toEntity(CreateSparkRequestDTO)`
- `toChildEntity(CreateSparkRequestDTO, parentId)`
- `toSparkDto(Spark)`
- `updateEntity(Spark, UpdateDTO)`
- `toTreeDto(Spark, List<SparkTreeDTO>)`

This ensures:
- consistency
- testability
- maintainability

No logic inside services or controllers.

---

# âš™ Service Layer

### SparkService
Defines operations:
- createSpark(dto)
- createChildSpark(parentId, dto)
- getSparkById(id)
- getChildren(id)
- getSparkTree(id)
- updateSpark(id, dto)
- deleteSparkCascade(id)
- deleteSparkReparent(id)
- deleteSparkLeaf(id)

### SparkServiceImpl

Implements all spark logic:

- Enforces title uniqueness
- Validates parent existence
- Retrieves children
- Builds recursive tree using repository
- Updates sparks
- Deletes with different strategies
- Maps to/from entities/DTOs
- Logs all important events

---

# ğŸ—„ Repository Layer

### SparkRepository
Extends MongoRepository and provides:

- `existsByTitle(String title)`
- `findByParentId(String parentId)`
- Default CRUD methods

---

# ğŸ’¥ Exception Layer

### SparkNotFoundException  
Thrown when spark id is missing.

### SparkAlreadyExistsException  
Thrown when title already exists.

### Handled globally  
GlobalExceptionHandler converts these into standardized ApiError objects.

---

# ğŸ§ª Testing

You have both:

## Controller Tests (WebMvcTest)
- test HTTP layer only
- mock service
- verify JSON responses

## Service Tests (Mockito)
- mock repository
- verify business logic
- verify mapping correctness
- verify repository interactions

---

# ğŸ“˜ OpenAPI Documentation

All spark endpoints defined in:

```
openapi.yaml
```

Contains schemas for:
- Spark
- SparkTree
- ApiError
- Create + Update request DTOs

Tags group everything under **Sparks**.

---

# ğŸ¯ Completed Spark Features

âœ” Create spark  
âœ” Create child spark  
âœ” Get spark by id  
âœ” Get children  
âœ” Get recursive spark tree  
âœ” Update spark  
âœ” Full DTO architecture  
âœ” Clean mapper usage  
âœ” Custom exceptions  
âœ” Logging  
âœ” Controller tests  
âœ” Service tests  
âœ” OpenAPI documentation

---

# ğŸš§ To Be Completed

â— Delete spark (3 modes):
- delete leaf
- delete cascade
- delete reparent
- full test coverage
- OpenAPI documentation

---

This document is enough for Copilot or any agent to fully understand the Spark feature module.
